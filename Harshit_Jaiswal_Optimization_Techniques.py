# -*- coding: utf-8 -*-
"""Harshit Jaiswal Optimization Techniques.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wyR846bsPuVeoqSS-zI1ezuwR0s7BQuA

# **Harshit Jaiswal**
**GitHub: [Click](https://github.com/harshitt018)**  
**LinkedIn: [Click](https://www.linkedin.com/in/harshit-jaiswal-a05835298/)**

# **Optimization Techniques Deep Learning**  
**Implementing all the optimizers commonly used in deep learning**

**Optimizers: ['SGD', 'Adam', 'RMSprop', 'Adagrad', 'Adadelta”, ‘Nadam']**

**-> Stochastic Gradient Descent (SGD)**

**-> Adam**

**-> RMSprop**

**-> Adagrad**

**-> Adadelta**

**-> Nadam**

# **Importing Required Libraries**
"""

from tensorflow.keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense
from tensorflow.keras.optimizers import SGD, Adam, RMSprop, Adagrad, Adadelta, Nadam
import matplotlib.pyplot as plt
from tensorflow.keras.utils import to_categorical
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import pyplot
import warnings
warnings.filterwarnings('ignore')

"""# **Example**"""

# Load and preprocess the MNIST dataset
(X_train, y_train), (X_test, y_test) = mnist.load_data()
X_train, X_test = X_train / 255.0, X_test / 255.0
X_train = X_train.reshape(-1, 784)
X_test = X_test.reshape(-1, 784)
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

# Model creation function
def create_model(optimizer):
    model = Sequential()
    model.add(Dense(64, input_dim=784, activation='relu'))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(10, activation='softmax'))
    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# List of optimizers
optimizers = [
    SGD(learning_rate=0.01),
    Adam(learning_rate=0.001),
    RMSprop(learning_rate=0.001),
    Adagrad(learning_rate=0.01),
    Adadelta(learning_rate=1.0),
    Nadam(learning_rate=0.001)
]
optimizer_names = ['SGD', 'Adam', 'RMSprop', 'Adagrad', 'Adadelta', 'Nadam']
history_dict = {}

# Train model with each optimizer and store the loss history
for optimizer, opt_name in zip(optimizers, optimizer_names):
    model = create_model(optimizer)
    print(f"Training with {opt_name}...")

    history = model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)
    history_dict[opt_name] = history.history['loss']

# Plotting the training loss for each optimizer
plt.figure(figsize=(10, 6))
for optimizer_name, loss in history_dict.items():
    plt.plot(loss, label=optimizer_name)

plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training Loss for different optimizers')
plt.legend()
plt.show()

"""# **Another Example**"""

# Function to build the model
def build_model(optimizer):
    model = Sequential()
    model.add(Dense(50, input_dim=784, activation='relu', kernel_initializer='he_uniform'))
    model.add(Dense(10, activation='softmax'))
    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    return model

# List of optimizer names
optimizer_names = ['Adadelta', 'Adagrad', 'Adam', 'RMSprop', 'SGD', 'Nadam']
optimizers = [Adadelta(), Adagrad(), Adam(), RMSprop(), SGD(), Nadam()]

# Loop over each optimizer and train the model
for i, opt_name in enumerate(optimizer_names):
    model = build_model(optimizers[i])
    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, verbose=0)

    # Evaluate the model
    train_acc = model.evaluate(X_train, y_train, verbose=0)
    test_acc = model.evaluate(X_test, y_test, verbose=0)

    # Plot the loss for training and validation sets
    pyplot.title(opt_name + ' Optimizer')
    pyplot.plot(history.history['loss'], label='train loss')
    pyplot.plot(history.history['val_loss'], label='test loss')
    pyplot.legend()
    pyplot.show()

    # Print the evaluation results
    print("Train Loss and Accuracy:", train_acc)
    print("Test Loss and Accuracy:", test_acc)
    print()